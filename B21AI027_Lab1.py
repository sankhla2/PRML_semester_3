# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mk84KheaJ_ECie_CJAt4iyXawTI3paaB

#Problem 1
"""

#Making data in a file named file.txt
import os
os.system("echo 1 > file.txt")
for i in range (2,10):
	os.system("echo "+str(i)+" >> file.txt")

with open("file.txt") as f:
	lst=[]
	for line in f:
		lst.append(int(line))

print(lst)

usr_input=input("Enter string to convert it to number:")
print(int(usr_input))

from datetime import datetime
datetimestr = '12/01/23 3:30:01'
datetime = datetime.strptime(datetimestr, '%m/%d/%y %H:%M:%S')
print(datetime)

import subprocess
subprocess.check_output("cat file.txt",shell=True)

lst=[1,2,1,2,3,4,2,1,1,1,1,1,1,3]
dict={}
for i in lst:
	if i in dict:
		dict[i]+=1
	else:
		dict[i]=1
print(dict)

from pandas.core.common import flatten
nst_lst=[[1],[1,2],[[1,2,3]],[[[1,2,3,4]]],[[[[1,2,3,4,5]]]]]
print(list(flatten(nst_lst)))

dict1={"a":1,"b":2,"c":3,"d":5}
dict2={"d":4,"e":5,"f":6}
dict={}
for i in dict1:
	dict[i]=dict1[i]
for i in dict2:
	if(i not in dict):
		dict[i]=dict2[i]
	else:
		dict[i]=dict[i]+dict2[i]

print(dict)

lst=[1,2,1,2,3,4,2,1,1,1,1,1,1,3]
dict={}
for i in lst:
	if i in dict:
		dict[i]+=1
	else:
		dict[i]=1
newlst=[]
for i in dict:
  newlst.append(i)

print(newlst)

dict={'a': 1, 'b': 2, 'c': 3, 'd': 9, 'e': 5, 'f': 6}
s=input("enter the key to search:")
if(s in dict):
	print("key is present and value of the key is:",dict[s])
else:
	print("key is not present")

"""#Problem 2"""

import numpy as np
mat1=np.array([[1,2,3],[4,5,6],[7,8,9]])
mat2=np.array([[10,11,12],[13,14,15],[16,17,18]])

print("first row of mat1:", mat1[0,:])
print("second column of mat2:",mat2[:,1])
print("matrix multiplication:\n",np.dot(mat1,mat2))
print("element wise multiplication:\n",mat1*mat2)
print("dot product between each column of first matrix and each column of second matrix:\n",np.dot(mat1.T, mat2))

"""#Problem 3"""

#Importing csv
import os
os.system("wget https://www.dropbox.com/s/uarfqnl296nebxt/Cars93.csv")

import pandas as pd
df=pd.read_csv("Cars93.csv")

# Question i
# (a) Model      - Nominal
# (b) Type       - Nominal
# (c) Max. Price - Ratio
# (d) Airbags    - Ordinal

#Question ii
def missingvalue(df):
    zdf=df.copy()
    return zdf.fillna(zdf.mean())

df=(missingvalue(df)).copy()

#Question iii
def reducenoise(df):
    zdf=df.copy()
    expected_types = {col: type(zdf.iloc[0][col]) for col in zdf.columns}
    noise_rows = []
    for i, row in zdf.iterrows():
        has_noise = False
        for col in zdf.columns:
            if(str(type(row[col]))=="<class 'int'>" and str(expected_types[col])== "<class 'numpy.int64'>"):
                continue
            if(str(type(row[col]))=="<class 'float'>" and str(expected_types[col])== "<class 'numpy.float64'>"):
                continue
            if type(row[col]) != expected_types[col]:
                zdf.at[i, col] = None
                has_noise = True
        if has_noise:
            noise_rows.append(i)
    zdf = zdf.drop(noise_rows)
    return zdf



df=(reducenoise(df)).copy()
df

#Question iv
import pandas as pd

def encode_categorical(df, ordinal_cols=None, onehot_cols=None):
    zdf=df.copy()
    if ordinal_cols is not None:
        for col in ordinal_cols:
            zdf[col] = zdf[col].astype('category')
            zdf[col] = zdf[col].cat.codes

    if onehot_cols is not None:
        zdf = pd.get_dummies(zdf, columns=onehot_cols)

    return zdf

lst=['Manufacturer','Model','DriveTrain','Man.trans.avail','Origin']
for col in df.columns:
    if df[col].dtype == object:
        lst.append(col)
df=(encode_categorical(df,['Type','AirBags'],lst)).copy()

#Question v
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
df
def normalize_features(df, method='joint', columns=None):
    zdf=df.copy()
    if method=='joint':
        scaler = MinMaxScaler()
        if columns is None:
            columns = zdf.columns
        zdf[columns] = scaler.fit_transform(zdf[columns])
        
    if method=='individual':
        if columns is None:
            columns = zdf.columns
        for col in columns:
            scaler = MinMaxScaler()
            zdf[col] = scaler.fit_transform(zdf[[col]])
    return zdf

df=normalize_features(df,'joint')

#Problem vi
def split_data(df, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):
    df = df.sample(frac=1).reset_index(drop=True)
    train_size = int(len(df) * train_ratio)
    val_size = int(len(df) * val_ratio)
    train = df[:train_size]
    val = df[train_size:train_size + val_size]
    test = df[train_size + val_size:]
    return train, val, test

split_data(df)

"""#Problem 4

##a
"""

import matplotlib.pyplot as plt
import numpy as np

x = np.linspace(-10, 10, 100)
y = 5*x+4
fig, ax = plt.subplots()
ax.plot(x, y)
ax.set_xlabel('x')
ax.set_ylabel('y')
plt.show()

"""##b"""

import matplotlib.pyplot as plt
import numpy as np

x = np.linspace(10, 100, 100)
y = np.log(x)
fig, ax = plt.subplots()
ax.plot(x, y)
ax.set_xlabel('x')
ax.set_ylabel('y')
plt.show()

"""##c

"""

import matplotlib.pyplot as plt
import numpy as np

x = np.linspace(-10, 10, 100)
y = x**2
fig, ax = plt.subplots()
ax.plot(x, y)
ax.set_xlabel('x')
ax.set_ylabel('y')
plt.show()

"""##d"""

import matplotlib.pyplot as plt
import numpy as np
data = np.array([[0,2],[1,3],[2,4],[3,5],[4,6]])
fig, ax = plt.subplots()
ax.scatter(data[:,0], data[:,1])
ax.set_xlabel('X-axis')
ax.set_ylabel('Y-axis')
plt.show()

"""#Problem 5

## Starter Code
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split as tts
from sklearn.linear_model import LogisticRegression as LR
from sklearn.tree import DecisionTreeClassifier as DTC
from sklearn.metrics import confusion_matrix as cm
from sklearn.metrics import precision_score as ps
from sklearn.metrics import recall_score as rs
from sklearn.metrics import f1_score as f1s
from sklearn.metrics import accuracy_score as acc

from warnings import simplefilter
from sklearn.exceptions import ConvergenceWarning
simplefilter("ignore", category=ConvergenceWarning)

# Dataset Reference :- https://www.kaggle.com/uciml/breast-cancer-wisconsin-data
import os
os.system("wget https://www.dropbox.com/s/uxg337i0vobh2da/Wisconsin_DataSet.csv")
data = pd.read_csv("./Wisconsin_DataSet.csv")
data

# Note :- There are many existing Encoders for converting String to Numeric Labels, but for convenience, we used Pandas.

condition_M = data.diagnosis == "M"
condition_B = data.diagnosis == "B"

data.loc[condition_M,"diagnosis"]=0
data.loc[condition_B,"diagnosis"]=1

data

Y = data.diagnosis.to_numpy().astype('int')                                     # Labels

X_data = data.drop(columns=["id","diagnosis","Unnamed: 32"])
X = X_data.to_numpy()                                                           # Input Features

X_data

user_prompt = 0.3
user_enable = False

x_train,x_test,y_train,y_test = tts(X,Y,test_size=user_prompt,shuffle=user_enable)

# Note :- Don't worry about the code snippet here, it is just to produce the predictions for the test data portion of each classifier

logistic_model = LR()
logistic_model.fit(x_train,y_train)
logistic_pred = logistic_model.predict(x_test)

decision_model = DTC()
decision_model.fit(x_train,y_train)
decision_pred = decision_model.predict(x_test)

"""##Evaluation Metrics (Inbulit v/s Scaratch)

### Confusion Matrix
"""

inbuilt_matrix_logistic = cm(y_test,logistic_pred)
inbuilt_matrix_decision = cm(y_test,decision_pred)

print("Confusion Matrix for Logistic Regression-based Predictions =>")
print(inbuilt_matrix_logistic)
print("Confusion Matrix for Decision Tree-based Predictions =>")
print(inbuilt_matrix_decision)

def confusion_matrix(y,algo_pred):
  TP=0
  TN=0
  FP=0
  FN=0
  for i in range (len(algo_pred)):
    if(y_test[i]==1 and algo_pred[i]==1):
        TP+=1
    elif(y_test[i]==0 and algo_pred[i]==0):
        TN+=1
    elif(y_test[i]==0 and algo_pred[i]==1):
        FP+=1
    elif(y_test[i]==1 and algo_pred[i]==0):
        FN+=1
  return np.array([[TN,FP],[FN,TP]])

print("###Using Scratch built confusion_matrix")
print("Confusion Matrix for Logistic Regression-based Predictions Using Scratch built =>")
print(confusion_matrix(y_test,logistic_pred))
print("Confusion Matrix for Decision Tree-based Predictions Using Scratch built =>")
print(confusion_matrix(y_test,decision_pred))

"""### Average Accuracy and Class-Wise Accuracy"""

inbuilt_acc_logistic = acc(y_test,logistic_pred)
inbuilt_acc_decision = acc(y_test,decision_pred)

print("Accuracy for Logistic Regression-based Predictions =>",str(inbuilt_acc_logistic*100)+"%")
print("Accuracy for Decision Tree-based Predictions =>",str(inbuilt_acc_decision*100)+"%")

def avg_accuracy(y,algo_pred):
  total_correct=0
  for i in range(len(algo_pred)):
    if(y[i]==algo_pred[i]):
      total_correct+=1
  return (total_correct/len(algo_pred))*100

print("###Using Scratch built avg_accuracy")
print("Accuracy for Logistic Regression-based Predictions Using Scratch built =>")
print(avg_accuracy(y_test,logistic_pred),"%")
print("Accuracy for Logistic Regression-based Predictions Using Scratch built =>")
print(avg_accuracy(y_test,decision_pred),"%")


def class_accuracy(y,algo_pred):
  TP=0
  TN=0
  FP=0
  FN=0
  for i in range (len(algo_pred)):
    if(y[i]==1 and algo_pred[i]==1):
        TP+=1
    elif(y[i]==0 and algo_pred[i]==0):
        TN+=1
    elif(y[i]==0 and algo_pred[i]==1):
        FP+=1
    elif(y[i]==1 and algo_pred[i]==0):
        FN+=1
  return (((TP/(TP+FN))*100)+((TN/(TN+FP))*100))/2

print("###Using Scratch built class_accuracy")
print("Classwise accuracy for Logistic Regression-based Predictions Using Scratch built =>")
print(class_accuracy(y_test,logistic_pred),"%")
print("Classwise accuracy for Logistic Regression-based Predictions Using Scratch built =>")
print(class_accuracy(y_test,decision_pred),"%")

"""### Precision"""

inbuilt_ps_logistic = ps(y_test,logistic_pred)
inbuilt_ps_decision = ps(y_test,decision_pred)

print("Precision for Logistic Regression-based Predictions =>",str(inbuilt_ps_logistic*100)+"%")
print("Precision for Decision Tree-based Predictions =>",str(inbuilt_ps_decision*100)+"%")

def precision(y,algo_pred):
  TP=0
  FP=0
  for i in range (len(algo_pred)):
    if(y[i]==1 and algo_pred[i]==1):
        TP+=1
    elif(y[i]==0 and algo_pred[i]==1):
        FP+=1
  return (TP/(TP+FP))*100

print("###Using Scratch built precision")
print("Precision for Logistic Regression-based Predictions Using Scratch built =>")
print(precision(y_test,logistic_pred),"%")
print("Precision for Logistic Regression-based Predictions Using Scratch built =>")
print(precision(y_test,decision_pred),"%")

"""### Recall"""

inbuilt_rs_logistic = rs(y_test,logistic_pred)
inbuilt_rs_decision = rs(y_test,decision_pred)

print("Recall for Logistic Regression-based Predictions =>",str(inbuilt_rs_logistic*100)+"%")
print("Recall for Decision Tree-based Predictions =>",str(inbuilt_rs_decision*100)+"%")

def recall(y,algo_pred):
  TP=0
  FN=0
  for i in range (len(algo_pred)):
    if(y[i]==1 and algo_pred[i]==1):
        TP+=1
    elif(y[i]==1 and algo_pred[i]==0):
        FN+=1
  return (TP/(TP+FN))*100

print("###Using Scratch built recall")
print("Recall for Logistic Regression-based Predictions Using Scratch built =>")
print(recall(y_test,logistic_pred),"%")
print("Recall for Logistic Regression-based Predictions Using Scratch built =>")
print(recall(y_test,decision_pred),"%")

"""### F-1 Score"""

inbuilt_f1s_logistic = f1s(y_test,logistic_pred)
inbuilt_f1s_decision = f1s(y_test,decision_pred)

print("F1-Score for Logistic Regression-based Predictions =>",str(inbuilt_f1s_logistic*100)+"%")
print("F1-Score for Decision Tree-based Predictions =>",str(inbuilt_f1s_decision*100)+"%")

def f1_score(y,algo_pred):
  return (2*precision(y,algo_pred)*recall(y,algo_pred))/(precision(y,algo_pred)+recall(y,algo_pred))

print("###Using Scratch built f1_score")
print("F1-Score for Logistic Regression-based Predictions Using Scratch built =>")
print(f1_score(y_test,logistic_pred),"%")
print("F1-Score for Logistic Regression-based Predictions Using Scratch built =>")
print(f1_score(y_test,decision_pred),"%")

"""### Sensitivity"""

def sensitivity(y,algo_pred):
  TP=0
  FN=0
  for i in range (len(algo_pred)):
    if(y[i]==1 and algo_pred[i]==1):
        TP+=1
    elif(y[i]==1 and algo_pred[i]==0):
        FN+=1
  return (TP/(TP+FN))*100

print("###Using Scratch built sensitivity")
print("Sensitivity for Logistic Regression-based Predictions Using Scratch built =>")
print(sensitivity(y_test,logistic_pred),"%")
print("Sensitivity for Logistic Regression-based Predictions Using Scratch built =>")
print(sensitivity(y_test,decision_pred),"%")

"""### Specificity"""

def specificity(y,algo_pred):
  TN=0
  FP=0
  for i in range (len(algo_pred)):
    if(y[i]==0 and algo_pred[i]==0):
        TN+=1
    elif(y[i]==0 and algo_pred[i]==1):
        FP+=1
  return (TN/(TN+FP))*100

print("###Using Scratch built specificity")
print("specificity for Logistic Regression-based Predictions Using Scratch built =>")
print(specificity(y_test,logistic_pred),"%")
print("specificity for Logistic Regression-based Predictions Using Scratch built =>")
print(specificity(y_test,decision_pred),"%")